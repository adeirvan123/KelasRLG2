{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, gym, numpy as np\n",
    "\n",
    "#Parameters\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.96\n",
    "epsilon = 0.9\n",
    "n_episodes = 10000\n",
    "max_steps = 100\n",
    "environment = gym.make('FrozenLake-v0')\n",
    "Q_matrix = np.zeros((environment.observation_space.n, environment.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64512f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    '''\n",
    "    To be used after Q table has been updated, returns an action\n",
    "    \n",
    "    Parameters:\n",
    "        \n",
    "        state - int - the current state of the agent \n",
    "        \n",
    "    :return: int\n",
    "    '''   \n",
    "    return np.argmax(Q_matrix[state, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploit_explore(prior_state, epsilon=epsilon, Q_matrix=Q_matrix):    \n",
    "    '''\n",
    "    One half of the exploit-explore paradigm that we will utilize \n",
    "    \n",
    "    Parameters \n",
    "        \n",
    "        prior_state - int  - the prior state of the environment at a given iteration\n",
    "        epsilon - float - parameter that we use to determine whether we will try a new or current best action \n",
    "        \n",
    "    :return: int\n",
    "    '''\n",
    "    \n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return environment.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q_matrix[prior_state, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f360650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_matrix(prior_state, observation , reward, action):\n",
    "    '''\n",
    "    Algorithm that updates the values in the Q table to reflect knowledge acquired by the agent \n",
    "    \n",
    "    Parameters \n",
    "    \n",
    "        prior_state - int  - the prior state of the environment before the current timestemp\n",
    "        observation - int  - the current state of the environment\n",
    "        reward - int - the reward yielded from the environment after an action \n",
    "        action - int - the action suggested by the epsilon greedy algorithm \n",
    "        \n",
    "    :return: None\n",
    "    '''\n",
    "    \n",
    "    prediction = Q_matrix[prior_state, action]\n",
    "    actual_label = reward + gamma * np.max(Q_matrix[observation, :])\n",
    "    Q_matrix[prior_state, action] = Q_matrix[prior_state, action] + learning_rate*(actual_label - prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_q_matrix(render=False, n_episodes=n_episodes):\n",
    "    '''\n",
    "    Directly implementing Q Learning (Greedy Epsilon) on the Frozen Lake Game\n",
    "    This function populations the empty Q matrix \n",
    "    Parameters \n",
    "    \n",
    "        prior_state - int  - the prior state of the environment before the current timestemp\n",
    "        observation - int  - the current state of the environment\n",
    "        reward - int - the reward yielded from the environment after an action \n",
    "        action - int - the action suggested by the epsilon greedy algorithm \n",
    "        \n",
    "    :return: None\n",
    "    '''    \n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        prior_state = environment.reset()\n",
    "        _ = 0\n",
    "        \n",
    "        while _ < max_steps:\n",
    "            \n",
    "            if render == True: environment.render()\n",
    "            action = exploit_explore(prior_state)  \n",
    "            observation, reward, done, info = environment.step(action)      \n",
    "            \n",
    "            update_q_matrix(prior_state=prior_state, \n",
    "                            observation=observation, \n",
    "                            reward=reward, \n",
    "                            action=action)\n",
    "            \n",
    "            prior_state = observation\n",
    "            _ += 1\n",
    "            \n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_frozen_lake(n_episodes):\n",
    "    \n",
    "    '''\n",
    "    Directly implementing Q Learning (Greedy Epsilon) on the Frozen Lake Game\n",
    "    This function uses the already populated Q Matrix and displays the game being used\n",
    "    \n",
    "    Parameters \n",
    "    \n",
    "        prior_state - int  - the prior state of the environment before the current timestemp\n",
    "        observation - int  - the current state of the environment\n",
    "        reward - int - the reward yielded from the environment after an action \n",
    "        action - int - the action suggested by the epsilon greedy algorithm \n",
    "        \n",
    "    :return: None\n",
    "    '''        \n",
    "        \n",
    "    for episode in range(n_episodes):\n",
    "        print('Episode: %s'%(episode+1))\n",
    "        prior_state = environment.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done: \n",
    "            environment.render()\n",
    "            action = choose_action(prior_state)\n",
    "            observation, reward, done, info = environment.step(action)\n",
    "            prior_state = observation\n",
    "            if reward == 0:\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                print('You have won on episode %s!'%(episode+1))\n",
    "                time.sleep(5)\n",
    "                os.system('clear')\n",
    "\n",
    "            if done and reward == -1:\n",
    "                print('You have lost this episode... :-/')\n",
    "                time.sleep(5)\n",
    "                os.system('clear')\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    populate_q_matrix(render=False)\n",
    "    play_frozen_lake(n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfdb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
